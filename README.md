This project aims to leverage the BLIP (Bootstrapped Language-Image Pretraining) model to identify and describe differences between two remote sensing (RS) images in the form of captions. The primary focus is fine-tuning the BLIP model for this task to achieve accurate and meaningful descriptions of changes.

Current Progress
* Fine-tuned the BLIP model for single-image captioning in the domain of remote sensing images.
* Future steps include fine-tuning and adapting the model for change detection captioning, where the differences between two RS images are described in natural language.
